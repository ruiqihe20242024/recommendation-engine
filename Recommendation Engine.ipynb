{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb6ba1e-b1b5-446c-ad20-dca48cc00462",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcef3f2f-641b-4530-848a-a39258eb513a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pyppeteer in /opt/anaconda3/envs/pyspark_env/lib/python3.8/site-packages (2.0.0)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /opt/anaconda3/envs/pyspark_env/lib/python3.8/site-packages (from Pyppeteer) (1.4.4)\n",
      "Requirement already satisfied: certifi>=2023 in /opt/anaconda3/envs/pyspark_env/lib/python3.8/site-packages (from Pyppeteer) (2024.8.30)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /opt/anaconda3/envs/pyspark_env/lib/python3.8/site-packages (from Pyppeteer) (7.0.1)\n",
      "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in /opt/anaconda3/envs/pyspark_env/lib/python3.8/site-packages (from Pyppeteer) (11.1.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /opt/anaconda3/envs/pyspark_env/lib/python3.8/site-packages (from Pyppeteer) (4.66.5)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /opt/anaconda3/envs/pyspark_env/lib/python3.8/site-packages (from Pyppeteer) (1.26.20)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in /opt/anaconda3/envs/pyspark_env/lib/python3.8/site-packages (from Pyppeteer) (10.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/envs/pyspark_env/lib/python3.8/site-packages (from importlib-metadata>=1.4->Pyppeteer) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/envs/pyspark_env/lib/python3.8/site-packages (from pyee<12.0.0,>=11.0.0->Pyppeteer) (4.11.0)\n",
      "zsh:1: command not found: pyppteer-install\n",
      "Requirement already satisfied: fpdf in /opt/anaconda3/envs/pyspark_env/lib/python3.8/site-packages (1.7.2)\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lower, when, col, count, desc\n",
    "from pyspark.sql.functions import concat_ws, lit, expr\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import rank\n",
    "!pip install Pyppeteer\n",
    "!pyppteer-install\n",
    "!pip install fpdf\n",
    "from fpdf import FPDF\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec41b9-f837-47f5-baf5-f2301fd73c34",
   "metadata": {},
   "source": [
    "**Initialize the Spark Session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "389b9dae-9b83-4ad1-a2fa-629f5c3afe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/16 23:10:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/16 23:10:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/09/16 23:10:58 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('Recommendation Engine') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1143aef-05e7-4251-98a8-3d1c2b9efc13",
   "metadata": {},
   "source": [
    "**Define Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b0e22-866a-4f78-8540-794b341312b9",
   "metadata": {},
   "source": [
    "Function1: unique_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c7ae4d2-3441-4cbe-9752-0875259ac125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/16 23:11:14 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "def unique_dataset(df):\n",
    "    \"\"\"\n",
    "    The unique_dataset function removes duplicates based on the SKU number, keeping the first occurrence.\n",
    "    @param df: The original DataFrame that contains all the SKUs.\n",
    "    @return: df_unique: A DataFrame that contains only the unique SKUs.\n",
    "    \"\"\"\n",
    "    # Drop duplicates based on the 'sku' column, keeping the first occurrence\n",
    "    df_unique = df.dropDuplicates(['sku'])\n",
    "    \n",
    "    # Return the updated DataFrame\n",
    "    return df_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f664b2f-175c-4c90-904b-b2eb927c5545",
   "metadata": {},
   "source": [
    "Function2: check_sku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a20efa66-da71-4055-b956-29b9ada9a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sku(df_unique, sku):\n",
    "    '''\n",
    "    The check_sku function checks whether a target SKU is present in the dataset.\n",
    "    @params: df_unique: the dataframe that contains only the unique SKUs; sku: the SKU entered by the user.\n",
    "    @return: a message showing whether the sku is present in the data.\n",
    "    '''\n",
    "    sku_df = df_unique.filter(lower(df_unique['sku']) == sku.lower())\n",
    "    if sku_df.count() > 0:\n",
    "        print(f\"{sku} is present in our dataset.\")\n",
    "        # Display the DataFrame result\n",
    "        df_result = sku_df.collect()  # You can use .show() for direct display, or .collect() for processing\n",
    "    else:\n",
    "        print(f\"{sku} is not present in our DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e4cfc-87c0-416d-9ec4-d611bed20c32",
   "metadata": {},
   "source": [
    "Function3: transposeRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b2f8fa0-ea87-4935-ae64-2f1789e849fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transposeRDD(df_unique,sku):\n",
    "    '''\n",
    "    The transposeRDD function transposes the dataframe and turn the dataframe into an RDD.\n",
    "    @params: df_unique: the dataframe that contains all the unique SKUs; sku: the SKU entered by the user.\n",
    "    @return: transpose_RDD: an RDD with each attributes as a column.\n",
    "    '''\n",
    "    transpose_RDD = df_unique.rdd.map(lambda x: (x[\"sku\"], x[\"attributes\"][0], x[\"attributes\"][1], x[\"attributes\"][2], x[\"attributes\"][3], x[\"attributes\"][4], x[\"attributes\"][5], x[\"attributes\"][6], x[\"attributes\"][7], x[\"attributes\"][8], x[\"attributes\"][9]))\n",
    "    return transpose_RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0366700-c79f-4ffa-a420-d9b76bf717e4",
   "metadata": {},
   "source": [
    "Function4: filterSku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5591fff2-c2b4-4ef7-88a8-1c568204a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterSku(transpose_RDD, sku):\n",
    "    '''\n",
    "    The filterSKU function filters out the target SKU.\n",
    "    @params: transpose_RDD: an RDD with each attributes as a column; sku: the SKU entered by the user.\n",
    "    @return: df_sku_view: a view of the data frame without the target SKU for comparison; target_row: the target SKU and its attributes.\n",
    "    '''\n",
    "    target_rdd = transpose_RDD.filter(lambda row: row[0] == sku)\n",
    "    filtered_rdd = transpose_RDD.filter(lambda row: row[0] != sku)\n",
    "    \n",
    "    columns = [\"Sku\", \"Att_a\", \"Att_b\", \"Att_c\", \"Att_d\", \"Att_e\", \"Att_f\", \"Att_g\", \"Att_h\", \"Att_i\", \"Att_j\"]\n",
    "\n",
    "    # Convert the filtered_rdd to a DataFrame with column names\n",
    "    filtered_df = filtered_rdd.toDF(columns)\n",
    "    filtered_df.createOrReplaceTempView(\"sku_view\")\n",
    "     \n",
    "    # Assuming rdd_result is an RDD containing a single row (target SKU row)\n",
    "    target_row = target_rdd.first()  # Extracts the tuple\n",
    "\n",
    "    # Load the sku_view into a DataFrame\n",
    "    df_sku_view = spark.sql(\"SELECT * FROM sku_view\")\n",
    "\n",
    "    return df_sku_view, target_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd7f0fb-e065-4903-8908-9fdb727e5320",
   "metadata": {},
   "source": [
    "Function5: SkuWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10cca78f-9a8e-4eeb-9956-e74f371d576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SkuWeight(df_sku_view,target_row):\n",
    "    '''\n",
    "    The SkuWeight function calculates the similarity that every SKU to the target SKU.\n",
    "    @params: df_sku_view: a view of the data frame without the target SKU for comparison; target_row: the target SKU and its attributes.\n",
    "    @return: df_with_weight: a data frame that has a column of the similarity that  every SKU to the target SKU.\n",
    "    '''\n",
    "     # Perform the comparison and create the new columns using direct indexing\n",
    "    df_with_scores = df_sku_view.withColumn(\"Va\", when(col(\"Att_a\") == target_row[1], 9).otherwise(-1)) \\\n",
    "    .withColumn(\"Vb\", when(col(\"Att_b\") == target_row[2], 8).otherwise(-1)) \\\n",
    "    .withColumn(\"Vc\", when(col(\"Att_c\") == target_row[3], 7).otherwise(-1)) \\\n",
    "    .withColumn(\"Vd\", when(col(\"Att_d\") == target_row[4], 6).otherwise(-1)) \\\n",
    "    .withColumn(\"Ve\", when(col(\"Att_e\") == target_row[5], 5).otherwise(-1)) \\\n",
    "    .withColumn(\"Vf\", when(col(\"Att_f\") == target_row[6], 4).otherwise(-1)) \\\n",
    "    .withColumn(\"Vg\", when(col(\"Att_g\") == target_row[7], 3).otherwise(-1)) \\\n",
    "    .withColumn(\"Vh\", when(col(\"Att_h\") == target_row[8], 2).otherwise(-1)) \\\n",
    "    .withColumn(\"Vi\", when(col(\"Att_i\") == target_row[9], 1).otherwise(-1)) \\\n",
    "    .withColumn(\"Vj\", when(col(\"Att_j\") == target_row[10], 0).otherwise(-1))\n",
    "\n",
    "    # Register the transformed DataFrame as a new view\n",
    "    df_with_scores.createOrReplaceTempView(\"sku_comparison_view\")\n",
    "    \n",
    "    # Create a new column \"weight\" by concatenating non-\"-1\" values and casting to integer\n",
    "    df_with_weight = df_with_scores.withColumn(\n",
    "        \"Weight\",\n",
    "        expr(\"CAST(concat_ws('', \" + \", \".join([f\"CASE WHEN {col} != -1 THEN CAST({col} AS STRING) ELSE '' END\" for col in ['Va', 'Vb', 'Vc', 'Vd', 'Ve', 'Vf', 'Vg', 'Vh', 'Vi', 'Vj']]) + \") AS INT)\")\n",
    "    )\n",
    "    return df_with_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c41fa6e-7e78-4722-874f-a24b6c379a08",
   "metadata": {},
   "source": [
    "Function6: GetSimilarWeightSku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eb3caee-366a-4178-9a38-a7a0992e7a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSimilarWeightSku(df_with_weight, sku):\n",
    "    '''\n",
    "    The GetSimilarWeightSku function oders the SKUs by their similarity to the target SKU.\n",
    "    @params: df_with_weight: a data frame that has a column of the similarity that  every SKU to the target SKU; sku: the SKU entered by the user.\n",
    "    @return: df_with_rank: a data frame that has a column of the rank of the similarity that  every SKU to the target SKU.\n",
    "    '''\n",
    "    # Define a window specification to rank by weight in descending order\n",
    "    window_spec = Window.orderBy(col(\"weight\").desc())\n",
    "    \n",
    "    # Assign a rank to every SKU by weight in descending order\n",
    "    df_with_rank = df_with_weight.withColumn(\"rank\", rank().over(window_spec))\n",
    "    \n",
    "    return df_with_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f94a247-cb18-455d-9514-0d4eeb425e58",
   "metadata": {},
   "source": [
    "Function7: TopSimilarSku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "266fef3c-bba2-4965-8190-f232c39aee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TopSimilarSku(df_with_rank, sku):\n",
    "    '''\n",
    "    The TopSimilarSku shows the top 10 similar SKUs to the target SKU.\n",
    "    @params: df_with_rank: a data frame that has a column of the rank of the similarity that  every SKU to the target SKU; sku: the SKU entered by the user.\n",
    "    @return: top_10_similar_skus: a data frame that shows the top 10 similar SKUs to the target SKU.\n",
    "    '''\n",
    "    # Order by rank in ascending order to get the top ranks\n",
    "    df_with_rank_ordered = df_with_rank.orderBy(\"rank\")\n",
    "    \n",
    "    # Get the top 10 rows based on the rank\n",
    "    df_with_rank_ordered_10 = df_with_rank_ordered.limit(10)\n",
    "\n",
    "    #keep only sku and attributes\n",
    "    #concatenate all attribute columns into a single string column\n",
    "    attribute_columns = [\"Att_a\", \"Att_b\", \"Att_c\", \"Att_d\", \"Att_e\", \"Att_f\", \"Att_g\", \"Att_h\", \"Att_i\", \"Att_j\"]\n",
    "    \n",
    "    # Combine all attributes into a single 'attribute' column\n",
    "    df_consolidated = df_with_rank_ordered_10.withColumn(\n",
    "        \"Attribute\", \n",
    "        concat_ws(\", \", *[col(attr) for attr in attribute_columns])\n",
    "    )\n",
    "    \n",
    "    # Select only the sku and the new attribute column\n",
    "    top_10_similar_skus = df_consolidated.select(\"Sku\", \"Weight\", \"Attribute\")\n",
    "\n",
    "    return top_10_similar_skus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0190274-e455-4a38-9e67-8614c4a0fed5",
   "metadata": {},
   "source": [
    "Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b83fb5dd-9433-41c5-beb8-432300c86441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sku-100 is present in our dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/16 23:12:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/16 23:12:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/16 23:12:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/16 23:12:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/16 23:12:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---------------------------------------------------------------------------------------------+\n",
      "|Sku      |Weight|Attribute                                                                                    |\n",
      "+---------+------+---------------------------------------------------------------------------------------------+\n",
      "|sku-9124 |98652 |att-a-13, att-b-5, att-c-9, att-d-4, att-e-5, att-f-8, att-g-5, att-h-7, att-i-3, att-j-13   |\n",
      "|sku-13907|87643 |att-a-11, att-b-5, att-c-4, att-d-4, att-e-12, att-f-7, att-g-12, att-h-11, att-i-12, att-j-6|\n",
      "|sku-1884 |86543 |att-a-8, att-b-5, att-c-12, att-d-4, att-e-5, att-f-7, att-g-12, att-h-13, att-i-6, att-j-4  |\n",
      "|sku-9347 |86432 |att-a-5, att-b-5, att-c-7, att-d-4, att-e-9, att-f-7, att-g-12, att-h-7, att-i-15, att-j-9   |\n",
      "|sku-13127|9864  |att-a-13, att-b-5, att-c-3, att-d-4, att-e-8, att-f-7, att-g-13, att-h-5, att-i-3, att-j-8   |\n",
      "|sku-11408|9862  |att-a-13, att-b-5, att-c-9, att-d-4, att-e-13, att-f-14, att-g-10, att-h-7, att-i-4, att-j-7 |\n",
      "|sku-3211 |9851  |att-a-13, att-b-5, att-c-13, att-d-1, att-e-5, att-f-8, att-g-10, att-h-8, att-i-9, att-j-9  |\n",
      "|sku-14868|9843  |att-a-13, att-b-5, att-c-15, att-d-13, att-e-9, att-f-7, att-g-12, att-h-10, att-i-1, att-j-5|\n",
      "|sku-18342|9840  |att-a-13, att-b-5, att-c-10, att-d-5, att-e-7, att-f-7, att-g-15, att-h-13, att-i-14, att-j-3|\n",
      "|sku-5077 |9830  |att-a-13, att-b-5, att-c-2, att-d-7, att-e-4, att-f-8, att-g-12, att-h-4, att-i-4, att-j-3   |\n",
      "+---------+------+---------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = glob.glob('**/dataset.json')\n",
    "    file_path = os.path.abspath(file_path[0])\n",
    "    df = spark.read.json(file_path)\n",
    "\n",
    "    target_sku = 'sku-100'\n",
    "    \n",
    "    # function1: Run the unique_dataset function to remove duplicates\n",
    "    df_unique = unique_dataset(df)\n",
    "    \n",
    "    # function2: Check if a specific SKU is present in the updated DataFrame\n",
    "    check_sku(df_unique, target_sku)  \n",
    "\n",
    "    # function3: transpose df into rdd with transposeRDD function\n",
    "    transpose_RDD = transposeRDD(df_unique, target_sku)\n",
    "\n",
    "    # function4: filter out the target sku\n",
    "    filterSku(transpose_RDD, target_sku)\n",
    "    df_sku_view, target_row = filterSku(transpose_RDD, target_sku)\n",
    "\n",
    "    # function5: calculate the weight\n",
    "    df_with_weight = SkuWeight(df_sku_view,target_row)\n",
    "\n",
    "    # function6: calculate similar weight\n",
    "    df_with_rank = GetSimilarWeightSku(df_with_weight,target_sku)\n",
    "\n",
    "    # function7: get top 10 similar SKUs using TopSimilarSku\n",
    "    top_10_similar_skus = TopSimilarSku(df_with_rank, target_sku)\n",
    "    \n",
    "    # return output: display the top 10 similar SKUs\n",
    "    top_10_similar_skus.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
